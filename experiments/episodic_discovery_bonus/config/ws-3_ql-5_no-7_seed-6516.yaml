general:
  discount_gamma: 0.5
  random_seed: 42
  use_cuda: True  # disable this when running on machine without cuda

  # Counting to explore (https://arxiv.org/pdf/1806.11525.pdf) bonuses; at most one should be True
  counting_to_explore_beta: 0.001
  state_history_capacity: 1000  # None means infinite
  use_cumulative_counting_bonus: False
  use_episodic_discovery_bonus: True

  # replay memory
  replay_memory_capacity: 500000  # adjust this depending on your RAM size
  replay_memory_priority_fraction: 0.25
  update_per_k_game_steps: 4
  replay_batch_size: 32

  # epsilon greedy
  epsilon_anneal_episodes: 300  # -1 if not annealing
  epsilon_anneal_from: 1.0
  epsilon_anneal_to: 0.2

checkpoint:
  experiment_tag: 'episodic_discovery_bonus-ws-3_ql-5_no-7_seed-6516'
  model_checkpoint_path: '../../experiments/episodic_discovery_bonus/models'
  load_pretrained: False  # during test, enable this so that the agent load your pretrained model
  pretrained_experiment_tag: 'name_of_your_previous_experiemnt_episode_k'
  save_frequency: 50

training:
  batch_size: 16
  nb_epochs: 501
  max_nb_steps_per_episode: 100  # after this many steps, a game is terminated
  optimizer:
    step_rule: 'adam'  # adam
    learning_rate: 0.001
    clip_grad_norm: 5

model:
  embedding_size: 64
  encoder_rnn_hidden_size: [192]
  action_scorer_hidden_dim: 128
  dropout_between_rnn_layers: 0.
